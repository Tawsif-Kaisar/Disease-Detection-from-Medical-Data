# -*- coding: utf-8 -*-
"""Disease_Detection from Medical Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16n2Kq7prsLWYxpUmWw4NfiyRvfM-0496
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import joblib

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/parkinsons.data')

# Display the first few rows
print(df.head(20))

# Check dataset structure
print(df.info())

# Check for missing values
print(df.isnull().sum())

# Display statistical summary
print(df.describe())

# Drop 'name' column as it's not useful
df = df.drop(columns=['name'])

# Define features (X) and target (y)
X = df.drop(columns=['status'])  # Features
y = df['status']                 # Target (0 or 1)

# Display shape of X and y
print(f"Features Shape: {X.shape}, Target Shape: {y.shape}")

from sklearn.model_selection import train_test_split

# Split dataset into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shape of training and testing sets
print(f"Training Data Shape: {X_train.shape}, Testing Data Shape: {X_test.shape}")

from sklearn.preprocessing import StandardScaler

# Initialize scaler
scaler = StandardScaler()

# Fit and transform the training data
X_train = scaler.fit_transform(X_train)

# Transform the testing data
X_test = scaler.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Calculate Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Display Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# testing for a new patient
new_patient = [11.992, 5.302, 14.997, 0.00784, 0.00007, 0.0370, 0.00554, 0.0140,
               0.1908, 0.64, 0.355, 211.093, 0.02184, 0.01908, 0.00143, 222.105, 0.426,
               0.141, 211.94, 0.00784, 0.7, 0.00370]

# Scale the new patient data
new_patient_scaled = scaler.transform([new_patient])

# Predict Parkinsonâ€™s status
predicted_status = model.predict(new_patient_scaled)
print("Predicted Status:", "Parkinson's" if predicted_status[0] == 1 else "Healthy")

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(kernel='linear'),
    "Neural Network": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Make predictions
    acc = accuracy_score(y_test, y_pred)  # Calculate accuracy
    print(f"{name} Accuracy: {acc:.2f}")

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier

# Train a Random Forest Model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get feature importance scores
feature_importances = rf.feature_importances_

# Sort and plot feature importances
indices = np.argsort(feature_importances)[::-1]  # Sort in descending order
plt.figure(figsize=(10,6))
sns.barplot(x=X.columns[indices], y=feature_importances[indices])
plt.xticks(rotation=90)
plt.xlabel("Feature Name")
plt.ylabel("Importance Score")
plt.title("Feature Importance Ranking")
plt.show()

# Select top 10 important features
important_features = X.columns[indices[:10]]
X_train_selected = X_train[:, indices[:10]]
X_test_selected = X_test[:, indices[:10]]

# Retrain Logistic Regression with selected features
log_reg = LogisticRegression()
log_reg.fit(X_train_selected, y_train)
y_pred = log_reg.predict(X_test_selected)
print(f"Logistic Regression Accuracy (Selected Features): {accuracy_score(y_test, y_pred):.2f}")

from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

# Initialize GridSearchCV
grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit model
grid_search.fit(X_train, y_train)

# Get best parameters
print("Best Parameters:", grid_search.best_params_)

# Evaluate best model
best_svm = grid_search.best_estimator_
y_pred = best_svm.predict(X_test)
print(f"Tuned SVM Accuracy: {accuracy_score(y_test, y_pred):.2f}")

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

# Initialize other models
models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Neural Network": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Make predictions
    acc = accuracy_score(y_test, y_pred)  # Calculate accuracy
    print(f"{name} Accuracy: {acc:.2f}")

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

# Retrain SVM on balanced data
best_svm.fit(X_train_balanced, y_train_balanced)
y_pred_balanced = best_svm.predict(X_test)
print(f"Tuned SVM Accuracy (Balanced Data): {accuracy_score(y_test, y_pred_balanced):.2f}")

import joblib

# Save the best model
joblib.dump(best_svm, "parkinsons_svm_model.pkl")

# Load the model whenever needed
loaded_model = joblib.load("parkinsons_svm_model.pkl")

# Test with new data
new_patient_data = X_test[0].reshape(1, -1)
prediction = loaded_model.predict(new_patient_data)
print("Predicted:", "Parkinson's" if prediction[0] == 1 else "Healthy")

# Default SVM Model (Before Tuning)
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

svm_default = SVC()  # No hyperparameter tuning
svm_default.fit(X_train, y_train)
y_pred_default = svm_default.predict(X_test)

# Accuracy before tuning
default_accuracy = accuracy_score(y_test, y_pred_default)
print(f"Accuracy Before Tuning: {default_accuracy:.2f}")

# Accuracy after tuning
tuned_accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy After Tuning: {tuned_accuracy:.2f}")

print("Best Hyperparameters Found:", grid_search.best_params_)

cv_results = pd.DataFrame(grid_search.cv_results_)
print(cv_results[['param_C', 'param_kernel', 'mean_test_score']])

import matplotlib.pyplot as plt

# Extract values
C_values = cv_results["param_C"].astype(float)
scores = cv_results["mean_test_score"]

# Plot
plt.figure(figsize=(8,5))
plt.plot(C_values, scores, marker='o', linestyle='dashed', color='b')
plt.xlabel("C Value")
plt.ylabel("Mean Accuracy")
plt.title("Effect of C on SVM Accuracy")
plt.xscale('log')  # Log scale for better visualization
plt.show()

import tensorflow as tf
from tensorflow import keras

# Build Neural Network Model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Binary classification
])

# Compile Model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train Model
model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test))

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import joblib

# Load dataset
df = pd.read_csv('/content/parkinsons.data')

# Drop the 'name' column
df = df.drop(columns=['name'])

# Define features (X) and target (y)
X = df.drop(columns=['status'])
y = df['status']

# Check feature correlation and remove highly correlated features
corr_matrix = X.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
threshold = 0.9  # Remove features with correlation > 0.9
high_corr_features = [column for column in upper.columns if any(upper[column] > threshold)]
X = X.drop(columns=high_corr_features)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA to reduce dimensionality
pca = PCA(n_components=0.95)  # Keep 95% variance
X_pca = pca.fit_transform(X_scaled)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=0, stratify=y)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

# Define models with hyperparameter tuning
param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
param_grid_rf = {'n_estimators': [50, 100, 200]}
param_grid_lr = {'C': [0.1, 1, 10]}

models = {
    "Logistic Regression": GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1),
    "SVM": GridSearchCV(SVC(), param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1),
    "Random Forest": GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
}

best_models = {}

for name, model in models.items():
    model.fit(X_train_balanced, y_train_balanced)
    best_models[name] = model.best_estimator_
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"{name} Best Params: {model.best_params_}")
    print(f"{name} Accuracy: {acc:.2f}\n")
    print(classification_report(y_test, y_pred))

# Neural Network Model
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, alpha=0.001, random_state=0, early_stopping=True)
mlp.fit(X_train_balanced, y_train_balanced)
y_pred_nn = mlp.predict(X_test)
print(f"Neural Network Accuracy: {accuracy_score(y_test, y_pred_nn):.2f}")

# Save the best models
for name, model in best_models.items():
    joblib.dump(model, f"best_{name.replace(' ', '_').lower()}.pkl")

# Save the scaler and PCA model
joblib.dump(scaler, "scaler.pkl")
joblib.dump(pca, "pca.pkl")

# Load and test a new patient
scaler = joblib.load("scaler.pkl")
pca = joblib.load("pca.pkl")
best_model = joblib.load("best_svm.pkl")  # Load the best model (e.g., SVM)

# Ensure new patient data has the same features as used in training
new_patient_data = np.array([11.992, 5.302, 14.997, 0.00784, 0.00007, 0.0370, 0.00554, 0.0140,
               0.1908, 0.64, 0.355, 211.093, 0.02184, 0.01908, 0.00143, 222.105, 0.426,
               0.141, 211.94, 0.00784, 0.7, 0.00370])

# Select only the features used in training
new_patient_data = new_patient_data[:X.shape[1]]

# Convert to numpy array and reshape
new_patient_processed = np.array(new_patient_data).reshape(1, -1)

# Preprocess the new sample
test_patient_scaled = scaler.transform(new_patient_processed)
test_patient_pca = pca.transform(test_patient_scaled)

# Predict
predicted_status = best_model.predict(test_patient_pca)
print("Predicted Status:", "Parkinson's" if predicted_status[0] == 1 else "Healthy")